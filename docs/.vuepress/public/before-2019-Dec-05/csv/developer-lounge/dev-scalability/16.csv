AuthorID;Author;Date;Content;Attachments;Reactions;
"232679450301431808";"blackjok3r#3181";"14-Jun-18 04:11 AM";"Hmm... I think maybe the variance I was getting too might have been the stats failing to submit.... for now I just put `&` at the end of the curl commands so the script can just continue if anything fails.";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 04:11 AM";"Testing this soon, if not, I have an idea to make `start=1` instead `start=timestamp_to_start_at`";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 04:12 AM";"and the pause mining will be calculated from that time, so all blasters start with a full mempool within  1 minute of each other they all start mining blocks.";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 04:13 AM";"the URL is passed by docker-compose so its shouldnt be too hard to have X of them üòÉ";"";"";
"371114647052615681";"Mylo#8306";"14-Jun-18 04:13 AM";"ok - if we need, we can check the curl command response code, and make some decisions to repost, or write to file and repost or something";"";"üëå (1)";
"371114647052615681";"Mylo#8306";"14-Jun-18 04:13 AM";"yup";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 04:14 AM";"The idea is to have 64 chains working perfectly with notarization... then just deploy as many of these 64 batch clusters into EKN to make up the rest of the chains needed to reach 1 million Tx/s";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 04:14 AM";"scales linear, so it should be easy to calibrate how much stuff we need.";"";"";
"371114647052615681";"Mylo#8306";"14-Jun-18 04:16 AM";"cool - i've got a service provider with physical servers that wants to get involved, so dockerizing is a winner here !";"";"";
"371114647052615681";"Mylo#8306";"14-Jun-18 04:19 AM";"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html";"";"";
"371114647052615681";"Mylo#8306";"14-Jun-18 04:19 AM";"10k requests per second for your txblaster starter";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 04:28 AM";"No way thats going to get breached.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 05:14 AM";"[slack] <smk762> Thanks guys. Csv might be ok, just gotta change some code at my end tonight to confirm. Be home in 3hrs and will have a look";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 05:30 AM";"[matrix] <mylo5ha5:matrix.org> remain flexible though, i can give you json - i was just massaging the data and thought to export to csv for spreadsheet ease";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 05:31 AM";"[matrix] <mylo5ha5:matrix.org> json is easier tbh";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 05:49 AM";"[slack] <smk762> Cool. I have backups of my backups so the biggest challenge in reverting is finding where what it used to he was :sleuth_or_spy:";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 05:49 AM";"[slack] <smk762> Just noticed firefox at work failing to load graph due to cors errors :(";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 05:50 AM";"[slack] <smk762> Strange. Its all under same domain.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 05:50 AM";"[slack] <smk762> Will troubleshoot once live data is linked, might need others to report if not working for them.";"";"";
"371114647052615681";"Mylo#8306";"14-Jun-18 06:21 AM";"CORS from where to where?  if it's different port it's a different origin";"";"";
"371114647052615681";"Mylo#8306";"14-Jun-18 06:21 AM";"e.g. 80 and 443";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 06:23 AM";"[slack] <smk762> Should both be port 80. Differnt folder but thats it";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 06:24 AM";"[slack] <smk762> http://cryptocartography.io/json/";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 06:25 AM";"[slack] <smk762> Could be company group policy or proxy or something. First PC/ browser ive seen the issue.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 06:27 AM";"[slack] <smk762> Its for the graph history. Realtime should be ok. If anyone is not seeing a graph when they load http://cryptocartography.io/txscl_vis/ please let me know";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 07:57 AM";"Does anyone know if this is a reasonable assumption about AWS?
64 vCPU instance is $3 per hour, so thats ~$0.047 (4.7c per hour) Per CPU.
If we need 8196 vCPU's  it would only be $385 per hour? 
If this is about right, then this will be no worries to run for a few hours?";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 08:21 AM";"[slack] <patchkez> @Mylo(KomodoPioneers) this is the curl Docker image you requested for testing requests against DynamoDB
https://github.com/patchkez/scaletest_containers/tree/master/dynamodb_requests_testing
you can start 8196 containers, or you can modify docker-compose and leave there only few chains. Curl is executed with 0.1s delay in the loop. If you need to deploy it on some faster HW, let me know, I will be glad to help.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 08:25 AM";"[slack] <patchkez> @blackjok3r count with only how many instances we will use. 8196/64 = 128.06. So ~129 instances is needed. Yes it makes ~380 per hour. There will be other costs included, e.g. dynamodb, network traffic, elastic volumes, ...";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 08:29 AM";"[slack] <system>";"https://cdn.discordapp.com/attachments/449949868904022036/456736852284538890/image.png";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 08:48 AM";"[matrix] <blackjok3r:matrix.org> test";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:04 AM";"```{ ""size"": 1587, ""height"": 31, ""time"": 1528964669, ""totaltx"": 1, ""ac"": ""TXSCL003"", ""mempooltx"": 0, ""mempoolMB"": 0 }
{ ""size"": 31807, ""height"": 32, ""time"": 1528964687, ""totaltx"": 135, ""ac"": ""TXSCL003"", ""mempooltx"": 6288, ""mempoolMB"": 1 }
{ ""size"": 1998957, ""height"": 33, ""time"": 1528966039, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 111008, ""mempoolMB"": 25 }
{ ""size"": 1998974, ""height"": 34, ""time"": 1528966075, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 102150, ""mempoolMB"": 23 }
{ ""size"": 1998967, ""height"": 35, ""time"": 1528966108, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 93292, ""mempoolMB"": 21 }
{ ""size"": 1998981, ""height"": 36, ""time"": 1528966131, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 91556, ""mempoolMB"": 20 }
{ ""size"": 1999038, ""height"": 37, ""time"": 1528966185, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 90131, ""mempoolMB"": 20 }
{ ""size"": 1998988, ""height"": 38, ""time"": 1528966330, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 90732, ""mempoolMB"": 20 }
{ ""size"": 1999020, ""height"": 39, ""time"": 1528966443, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 90625, ""mempoolMB"": 20 }
{ ""size"": 1998976, ""height"": 40, ""time"": 1528966486, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 85138, ""mempoolMB"": 19 }
{ ""size"": 1998994, ""height"": 41, ""time"": 1528966536, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 78529, ""mempoolMB"": 17 }
{ ""size"": 1998989, ""height"": 42, ""time"": 1528966641, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 75517, ""mempoolMB"": 17 }
{ ""size"": 1998987, ""height"": 43, ""time"": 1528966664, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 72074, ""mempoolMB"": 16 }
{ ""size"": 1999047, ""height"": 44, ""time"": 1528966752, ""totaltx"": 8859, ""ac"": ""TXSCL003"", ""mempooltx"": 66378, ""mempoolMB"": 
```";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:04 AM";"üòÑ";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:04 AM";"That appears to be the last thing required as far as the docker image goes.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 09:04 AM";"[slack] <CrypToon> https://twitter.com/allanjunli/status/1007171631972864000?s=21";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 09:05 AM";"[slack] <CrypToon> Anyone have some input to this Q?";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:05 AM";"Each chains uses 1 vCPU";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:05 AM";"nothing at all";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:05 AM";"1 CPU per chain";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:06 AM";"you can connect as many ""nodes"" to it as your like if you have the money/resources to deploy them all.";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:06 AM";"I have 64 chains on a single 4 core CPU for the Notary Nodes.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 09:06 AM";"[matrix] <jl777:matrix.org> if anybody claims that a blockchain with nodes on AWS  wont work with normal nodes, I would like to know what their basis for this claim is? AWS nodes are like normal nodes as far as blockchain goes";"";"üëå (1)";
"455741312273219595";"jl777c#5810";"14-Jun-18 09:09 AM";"the bandwidth used for any single blockchain is as a normal blockchain is";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:09 AM";"2mb per minute.";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:09 AM";"per chain.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 09:11 AM";"[matrix] <jl777:matrix.org> so it is easy to rebut that twitter";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 09:15 AM";"[slack] <CrypToon> Ok ill answer in a bit if someone doesnt beat me to it need to run som errands first";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jun-18 09:35 AM";"New trigger is working great.. there is still some variation between the two tests due to block time variations... but we can just increase the amount of blocks in the first test by some more.. the Stats website can tell the difference between  1 payment and 100 payment blocks anyway so it shouldn't really matter if we get a little bit of crossover. 

if it proves to be an issue once I can test with the stats site, I can adjust the gaps between the tests. Main thing is I managed to get single payment to run for 15 minutes with full blocks for 20 blocks, using just a singel CPU per chain. üòÑ";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 09:53 AM";"[slack] <CrypToon> Hey, not really sure what you mean by this question and why it is relevant, there was one vCPU per chain. You can connect as many nodes to is as you like if you have the money/resources to deploy them all. The bandwidth is approx 2mb per minute per chain, so it‚Äôs not really an issue.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 09:53 AM";"[slack] <CrypToon> Something like this?";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jun-18 12:29 PM";"payments per tx calc could be better, just based on a couple of reference values at the moment. It can detect where all tx in block are 100 or all tx in block are 20, but mixed payment/tx blocks are less relaiable.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jun-18 12:30 PM";"if some chains are on 20 and others are on 100, it's all good";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 01:20 PM";"[matrix] <ross.t:matrix.org> https://medium.com/@siddharth.sitpure/a-closer-look-at-ripples-blockchain-technology-and-xrp-9e036e1bf019";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 02:14 PM";"[slack] <blackjok3r> The blocks only have 1 or 100.. there is no in between on each chain.";"";"";
"455021680935436290";"Gateway#3763";"14-Jun-18 02:15 PM";"[slack] <blackjok3r> but the chains them selves may overlap .. so 1 chain has 1 payments and another has 100.";"";"";
"371114647052615681";"Mylo#8306";"15-Jun-18 02:20 AM";"@smk762 -  dracocanis ominator had 360 visitors to my last blog entry in last 12 hours re: scaling update.  30 went onto cryptocartography.  better be good :troll: hehe";"";"";
"455021680935436290";"Gateway#3763";"15-Jun-18 02:23 AM";"[slack] <smk762> I hope so. Added a bit of text to mention its random data right now. Will parse csv and replace random history where available over the wkend";"";"";
"455021680935436290";"Gateway#3763";"15-Jun-18 02:27 AM";"[slack] <xrobesx> Great work guys. It was a nice blog post mylo";"";"";
"371114647052615681";"Mylo#8306";"15-Jun-18 02:53 AM";"thanks mate - yeah gotta keep ppl in the loop, keep our dev in the open etc. because transparency will win for adoption";"";"";
"371114647052615681";"Mylo#8306";"15-Jun-18 04:09 AM";"hey @patchkez would i be able to use a python threadpool to issue curl requests to load test my amz rest api pretty easily? e.g. https://www.codementor.io/lance/simple-parallelism-in-python-du107klle
i used to do this in java, but don't know python - but it looks simple enough to hack at";"";"";
"371114647052615681";"Mylo#8306";"15-Jun-18 04:09 AM";"inside the containers - does docker grab threads as it needs?";"";"";
"455021680935436290";"Gateway#3763";"15-Jun-18 06:43 AM";"[matrix] <patchkez:matrix.org> @mylo5ha5:matrix.org: did not try any python multithreaded app in container, but I assune it will work. containers are just isolated processes. did you try docker images + docker-compose I prepared for you?";"";"";
"232679450301431808";"blackjok3r#3181";"15-Jun-18 08:07 AM";"You can spawn multiple process inside a container no problem. I did it for the TxBlaster, it uses komodod + blocknotify and and pre-compiled marketmaker.";"";"";
"455021680935436290";"Gateway#3763";"16-Jun-18 05:15 AM";"[slack] <smk762> any explorers for the asset chains at the moment? want to review a few reference blocks to confirm logic for blocksize  => payment per tx detction.";"";"";
"455021680935436290";"Gateway#3763";"16-Jun-18 05:38 AM";"[slack] <smk762> @Mylo(KomodoPioneers) in the json / csv, is the `mempooltx` value equivalent to sum of payment counts within `totaltx` value? if so I'll use that rather than calc on my end.";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 06:43 AM";"```block=$(komodo-cli -ac_name=$chain getblock $HEIGHT)
    mempool=$(komodo-cli -ac_name=$chain getmempoolinfo)
    blockinfo=$(echo $block | jq '{size, height, time}')
    totaltx=$(echo $block | jq '.tx | length')
    mempooltx=$(echo $mempool | jq -r .size)
    mempoolMB=$(( $(echo $mempool | jq -r .bytes) / 1000000 ))
    RESULT=$(echo $blockinfo | jq --argjson mempooltx $mempooltx --argjson mempoolMB $mempoolMB --argjson totaltx $totaltx --arg chain $chain '. += {""totaltx"":$totaltx, ""ac"":$chain, ""mempooltx"":$mempooltx, ""mempoolMB"":$mempoolMB}')```";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 06:43 AM";"Thats the JSON being pushed to the DB.";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 06:44 AM";"If you need anything else please let me know and I can calculate it on this end, as this node has very low load as it is, and there is one for every chain, it will keep everything nice and distributed.";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 06:46 AM";"I think you want to totally ignore mempool info, as I only put that in there for my own debuging of TxBlaster.";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 06:46 AM";"And  unless you put something to show mempool size on stats website, that info is redundant and I will remove it for real test.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"16-Jun-18 07:02 AM";"yeah looks like mempool stuff not needed on my end. can calc payment per tx from block size, or simply from `totaltx`.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"16-Jun-18 07:02 AM";"8859 means singles";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"16-Jun-18 07:03 AM";"557 means hundreds";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"16-Jun-18 07:04 AM";"a little fuzzy inbetween, but when we're at capacity it is pretty straight forward";"";"";
"455021680935436290";"Gateway#3763";"16-Jun-18 07:05 AM";"[matrix] <jl777:matrix.org> bytes per tx, calculate this and it will be easy to differentiate. dont just use total tx per block";"";"";
"455021680935436290";"Gateway#3763";"16-Jun-18 07:06 AM";"[matrix] <jl777:matrix.org> size of block divided by numtx";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 07:08 AM";"THe is no possible way that a single chain can mix blocks of 1 payment and 100 payment.";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 07:09 AM";"but there could be some chains on 1 and some one 100, but it is not much crossover.";"";"";
"455021680935436290";"Gateway#3763";"16-Jun-18 07:10 AM";"[matrix] <jl777:matrix.org> yes but it is possible to not have a full block";"";"";
"455021680935436290";"Gateway#3763";"16-Jun-18 07:11 AM";"[matrix] <jl777:matrix.org> so if no crossover, my method works regardless of how many tx in a block";"";"üëå (1)";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 07:13 AM";"Yes towards the end of the single payment there will be blocks that are not full. 100 payment, I am yet to see a block that is not full.";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 07:13 AM";"But calculating it per byte or pre TX is the way to go.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"16-Jun-18 07:14 AM";"all good then. should have the csv parsed and aggregated soon. looking at the data there are some good indicators.";"";"";
"232679450301431808";"blackjok3r#3181";"16-Jun-18 07:15 AM";"sweet, so all we need now is a way for the DB to push the required data to you. The last test I ran with 64 chains I lef the AWS on stopped, so its no issue to load it up and run another test any time. No need to rebuild docker image. I do have discord on my phone, so I can be pinged here, if you guys need me.";"";"ü§ò (1)";
"371114647052615681";"Mylo#8306";"16-Jun-18 10:13 PM";"@patchkez not yet.  Day off with missus and fiat day";"";"";
"378000766994087936";"J-Mack#9626";"18-Jun-18 04:32 AM";"Hey guys!";"";"";
"378000766994087936";"J-Mack#9626";"18-Jun-18 04:32 AM";"Quick Question -";"";"";
"378000766994087936";"J-Mack#9626";"18-Jun-18 04:32 AM";"If u hold in paper wallet do u still get rewards?";"";"";
"456226577798135808";"Deleted User#0000";"18-Jun-18 04:33 AM";"@J-Mack #support";"";"";
"455021680935436290";"Gateway#3763";"18-Jun-18 10:13 AM";"[matrix] <mylo5ha5:matrix.org> @smk782 crypto cartography site has the < > the wrong way i think";"";"";
"455021680935436290";"Gateway#3763";"18-Jun-18 11:35 AM";"[slack] <Blocktech> where can we follow the work of the devs about scaling ? ( l liked to follow discution on slack to inform myself, but now devs aren't anymore on slack )";"";"";
"232679450301431808";"blackjok3r#3181";"18-Jun-18 12:25 PM";"discord is what I am using now.";"";"";
"371114647052615681";"Mylo#8306";"18-Jun-18 01:26 PM";"my last update of what we've done https://i.mylomylo.com/komodo-blockchain-scaling-1-million-tx-per-second-stats-prep-etc/ and this discord channel probably best - i have abandoned slack, others may be still in there and connected by bridge like yourself";"";"";
"455021680935436290";"Gateway#3763";"18-Jun-18 02:56 PM";"[slack] <smk762> i'll change to words to avoid confusion";"";"";
"354615658148790275";"sœÜldat#8733";"19-Jun-18 09:04 AM";"@Mylo  That's exactly the kind of article we need. (clear and easy)
I received many impressions and messages as well as more than 10k views in two days.
Nice work";"";"";
"371114647052615681";"Mylo#8306";"19-Jun-18 09:31 AM";"thanks for the feedback @sœÜldat - blog entry got well over 1k visitors to that article over 3 or 4 days, 15% roughly went onto cryptocartography, 10% to pioneers which shows the article got read to the bottom on many occasions.
steve lee provided the aws architecture diagram - wrote the article about an hour after the diagram went public (which is about 20 mins after i first saw it) üëç";"";"";
"371114647052615681";"Mylo#8306";"19-Jun-18 09:33 AM";"i haven't logged into linkedin for a few days, but simply reaching out to the new contacts with info/links relevant to their profile page has opened up eyes and opportunities";"";"";
"371114647052615681";"Mylo#8306";"19-Jun-18 09:35 AM";"being able to scale tx/s is good for dApp developers.
being able to dPoW to btc is good for coins & ecosystems
kmd will be a good partner to many üòÑ";"";"";
"371114647052615681";"Mylo#8306";"19-Jun-18 09:41 AM";"to all the holders of kmd, every 100-300kmd held is 1 year of notarisation for a dApp in the future.  these aren't just opportunities to profit in the case of kmd price rising, but an opportunity to learn how to apply the knowledge of the ecosystem in future dealings with your own partners in projects - and the team is here to help with the learning as we build resources for this technology to be available to anyone.";"";"üëç (1)";
"303794669945618442";"PTYX#6840";"19-Jun-18 10:57 AM";"Agreed, I've been concentrating on LinkedIn recently";"";"";
"303794669945618442";"PTYX#6840";"19-Jun-18 10:58 AM";"Even got my own LinkedIn wizard. Lol albeit I was hesitant at first but it's worth it.";"";"";
